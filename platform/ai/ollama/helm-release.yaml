apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: ollama
  namespace: ai-stack
spec:
  interval: 5m
  chart:
    spec:
      chart: ollama
      version: ">=0.1.0"
      sourceRef:
        kind: HelmRepository
        name: ollama
        namespace: flux-system
  values:
    environment: ${ENVIRONMENT}
    image:
      repository: ollama/ollama
      tag: latest
    replicaCount: 1
    resources:
      requests:
        memory: ${OLLAMA_MEMORY_REQUEST:-2Gi}
        cpu: ${OLLAMA_CPU_REQUEST:-1000m}
      limits:
        memory: ${OLLAMA_MEMORY}  # From environment
        cpu: ${OLLAMA_CPU:-4000m}
        nvidia.com/gpu: "1"
    persistence:
      enabled: true
      size: ${OLLAMA_STORAGE:-50Gi}
      mountPath: /root/.ollama
    models:
      - name: llama2
    service:
      type: ClusterIP
      port: 11434
    monitoring:
      enabled: true
      serviceMonitor:
        enabled: true
    env:
      - name: OLLAMA_MODELS
        value: "llama2,codellama,mistral"
      - name: NVIDIA_VISIBLE_DEVICES
        value: "all"
    metrics:
      enabled: true
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "11434"
        prometheus.io/path: "/metrics"
    probes:
      liveness:
        enabled: true
        path: /api/health
        port: 11434
        initialDelaySeconds: 30
        periodSeconds: 10
      readiness:
        enabled: true
        path: /api/health
        port: 11434
        initialDelaySeconds: 30
        periodSeconds: 10
